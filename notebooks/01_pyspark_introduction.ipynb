{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_pyspark_introduction.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "lcvzXjMtREF9"
      ],
      "authorship_tag": "ABX9TyOVnawgUzqrutux87Na09tB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ralsouza/apache_spark_real_time_analytics/blob/master/notebooks/01_pyspark_introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcvzXjMtREF9",
        "colab_type": "text"
      },
      "source": [
        "# 1. Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCutjJI1PCIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# instalar as dependências\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFxI1rhTP0EY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# configurar as variáveis de ambiente\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        " \n",
        "# tornar o pyspark \"importável\"\n",
        "import findspark\n",
        "findspark.init('spark-2.4.4-bin-hadoop2.7')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na5gbGcUQtCy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# iniciar uma sessão local e importar dados do Airbnb\n",
        "# from pyspark.sql import SparkSession\n",
        "# sc = SparkSession.builder.master('local[*]').getOrCreate()\n",
        " \n",
        "# download do http para arquivo local\n",
        "# !wget --quiet --show-progress http://data.insideairbnb.com/brazil/rj/rio-de-janeiro/2019-07-15/visualisations/listings.csv\n",
        " \n",
        "# carregar dados do Airbnb\n",
        "# df_spark = sc.read.csv(\"./listings.csv\", inferSchema=True, header=True)\n",
        " \n",
        "# ver algumas informações sobre os tipos de dados de cada coluna\n",
        "# df_spark.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Haja9PCXRJ2S",
        "colab_type": "text"
      },
      "source": [
        "# 2. Pyspark Introduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lznInPjYUzci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext(\"local[*]\", \"My First App\")\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFOBm-HVYXio",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Stoping Context\n",
        "# sc.stop()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_s6nwH-RR3S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "56cbea12-b02d-481e-b4a0-9e1e18b048fb"
      },
      "source": [
        "import sys\n",
        "print(sys.version)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.6.9 (default, Apr 18 2020, 01:56:04) \n",
            "[GCC 8.4.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCWZKkqTRXex",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c3ef844b-ff64-4480-d00b-0d50f664872b"
      },
      "source": [
        "# Print session context (Spark Context)\n",
        "print(sc)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<SparkContext master=local[*] appName=My First App>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nd4LnQfRhod",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a2108c0-4de2-40c7-8ea1-2e9b202e4bc6"
      },
      "source": [
        "# Version context\n",
        "print(sc.version)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ERw3iWdRxHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testing Spark and creating a RDD\n",
        "# We can't put a Python list in a Spark cluster, it's needed to convert it to \n",
        "# a RDD\n",
        "lst = [25,90,81,37,776,3320]\n",
        "test_data = sc.parallelize(lst,10)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sn1Lky6S2d1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# What does sc.parallelize?\n",
        "?sc.parallelize\n",
        "\n",
        "# Signature: sc.parallelize(c, numSlices=None)\n",
        "# Docstring:\n",
        "# Distribute a local Python collection to form an RDD (Resilient Distribuited \n",
        "# Dataset). \n",
        "# Using xrange\n",
        "# is recommended if the input represents a range for performance.\n",
        "\n",
        "# >>> sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n",
        "# [[0], [2], [3], [4], [6]]\n",
        "# >>> sc.parallelize(xrange(0, 6, 2), 5).glom().collect()\n",
        "# [[], [0], [], [2], [4]]\n",
        "# File:      /content/spark-2.4.4-bin-hadoop2.7/python/pyspark/context.py\n",
        "# Type:      method"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcpdaHVFZWk4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef2f73b4-cb87-4579-fd7f-d44a58b5a91d"
      },
      "source": [
        "# Check data type\n",
        "type(test_data)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.rdd.RDD"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxHyUSuLZoPL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "12c90854-53c6-4dda-9975-00a5cde8d439"
      },
      "source": [
        "# Counting data\n",
        "test_data.count()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ln_Yv0gvZvUX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5bba121d-aff9-4be2-a20b-83dd33b28b14"
      },
      "source": [
        "# List values\n",
        "test_data.collect()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[25, 90, 81, 37, 776, 3320]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qhkP2AVab5i",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7cnZn76ZxpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}