{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_spark_streaming_twitter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1k808iriwznjrjvA82P_caPEP0_ZhuCIR",
      "authorship_tag": "ABX9TyPx0Aj7yh0dWh+kJYaFY1Uy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ralsouza/apache_spark_real_time_analytics/blob/master/notebooks/spark_streaming_twitter/01_spark_streaming_twitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_fEra2hzIM5"
      },
      "source": [
        "# PySpark Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QjpiQscwRZE"
      },
      "source": [
        "!apt-get update"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8k0XMjMzzMih"
      },
      "source": [
        "# Install the dependencies\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FaZqyypzOSQ"
      },
      "source": [
        "# Environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        " \n",
        "# tornar o pyspark \"import√°vel\"\n",
        "import findspark\n",
        "findspark.init('spark-2.4.4-bin-hadoop2.7')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XL0n6A3izPzT"
      },
      "source": [
        "# Libraries and Context Setup\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "\n",
        "# Instance Spark Session\n",
        "spark = SparkSession.builder.master('local').appName('My-SparkSQL').getOrCreate()\n",
        "\n",
        "# Create the SQL Context\n",
        "sqlContext = pyspark.SQLContext(sc)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRPPQv2-zRl9",
        "outputId": "f6b55013-e031-42d9-a6a4-b63d163d882d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Check context\n",
        "print(sc)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<SparkContext master=local[*] appName=pyspark-shell>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrnfEnJNeuIY"
      },
      "source": [
        "## Other packages to streaming - Twitter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMRD8wO1ereD"
      },
      "source": [
        "!pip install requests_oauthlib\n",
        "!pip install twython\n",
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7_kJi9YfcBE"
      },
      "source": [
        "## Install Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RlliEh3e7Yk"
      },
      "source": [
        "from pyspark.streaming import StreamingContext\n",
        "from requests_oauthlib import OAuth1Session\n",
        "from operator import add\n",
        "from time import gmtime, strftime\n",
        "import requests\n",
        "import time\n",
        "import string \n",
        "import ast"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq6MlEAug0S_"
      },
      "source": [
        "## Install NLTK modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0_UEkBrfkYp"
      },
      "source": [
        "import nltk\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.sentiment import SentimentAnalyzer\n",
        "from nltk.corpus import subjectivity\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment.util import *"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qc26fUIAhaFw"
      },
      "source": [
        "# Update frequency\n",
        "BATCH_INTERVAL = 5"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9RP6bKLhvL8"
      },
      "source": [
        "# Making the StreamingContext\n",
        "ssc = StreamingContext(sc,batchDuration=BATCH_INTERVAL)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRl64bOZ7Oho"
      },
      "source": [
        "An essencial part to create a sentiment analysis algorithm, such as any data mining algorithm, is to have a comprehensive data or \"corpus\" to learn, as well as a dataset to test and to ensure it perfectly meet the requeriments.\n",
        "\n",
        "It allows you to adjust the algorithm to deduce better (or more accurate) natural language characteristics that could be extracted from the text and that will contribuite to the sentiment classification, instead of using a generic approach.\n",
        "\n",
        "We will take as a work base a train dataset provided by Michigan University, to Kaggle competitions -  https://inclass.kaggle.com/c/si650winter11.\n",
        "\n",
        "This dataset contains 1.578.627 classified tweets and each row is marked as:\n",
        "* 1 with regard positive sentiment\n",
        "* 0 with regard negative sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBXlspFHqXdD"
      },
      "source": [
        "rdd_sent = sc.textFile('/content/drive/My Drive/Colab Notebooks/08-apache-spark/data/dataset_analise_sentimento.csv')"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J10VlebYhoT",
        "outputId": "cf221ec7-b894-49ad-c78a-69b8f8e766dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "rdd_sent.take(5)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ItemID,Sentiment,SentimentSource,SentimentText',\n",
              " '1,0,Sentiment140,                     is so sad for my APL friend.............',\n",
              " '2,0,Sentiment140,                   I missed the New Moon trailer...',\n",
              " '3,1,Sentiment140,              omg its already 7:30 :O',\n",
              " \"4,0,Sentiment140,          .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Zt0VnJmqB7v"
      },
      "source": [
        "# Removing header\n",
        "header = rdd_sent.take(1)[0]\n",
        "dataset = rdd_sent.filter(lambda row: row != header)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YERbTX-RaMw3",
        "outputId": "6e85b52d-7720-4e72-e4a6-59005974dd42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "header"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ItemID,Sentiment,SentimentSource,SentimentText'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiT9pzcSWN9v",
        "outputId": "10d75e97-2323-4a7e-ad01-24a0af3c60bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "dataset.take(5)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1,0,Sentiment140,                     is so sad for my APL friend.............',\n",
              " '2,0,Sentiment140,                   I missed the New Moon trailer...',\n",
              " '3,1,Sentiment140,              omg its already 7:30 :O',\n",
              " \"4,0,Sentiment140,          .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...\",\n",
              " '5,0,Sentiment140,         i think mi bf is cheating on me!!!       T_T']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCX5m4QJqSPD",
        "outputId": "476b4fb5-691b-4400-e2ee-b2bd9fb4d59c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(dataset)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.rdd.PipelinedRDD"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h98Fjtz2rKIR"
      },
      "source": [
        "# This function splits the columns in each row, creating a tuple and removing \n",
        "# the punctiation\n",
        "def get_row(row):\n",
        "  row = row.split(\",\")\n",
        "  sentiment = row[1]\n",
        "  tweet = row[3].strip()\n",
        "  translator = str.maketrans({key: None for key in string.punctuation})\n",
        "  tweet = tweet.translate(translator)\n",
        "  tweet = tweet.split(' ')\n",
        "  tweet_lower = []\n",
        "  for word in tweet:\n",
        "    tweet_lower.append(word.lower())\n",
        "  return (tweet_lower, sentiment)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_DeQOVvzVvu"
      },
      "source": [
        "# Apply the function in each row in the dataset\n",
        "ds_train = dataset.map(lambda row: get_row(row))"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htOq0G8gzmyE"
      },
      "source": [
        "# Create an object SentimentAnalyser\n",
        "sentiment_analyzer = SentimentAnalyzer()"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2qz_rH4z53c",
        "outputId": "cdc25c36-700e-41e5-a60d-95f6621d4905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Download the stopwords package - Need approximately 5GB on disk\n",
        "# https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
        "# Full download nltk.download()\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_430Io5g9nAS",
        "outputId": "0b589e84-934c-46c9-910d-64eb66383b4e",
        "colab": {
          "resources": {
            "http://localhost:8080/content/drive/My%20Drive/Colab%20Notebooks/08-apache-spark/images/ntlkdata.png": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "status": 404,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(url='/content/drive/My Drive/Colab Notebooks/08-apache-spark/images/ntlkdata.png')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"/content/drive/My Drive/Colab Notebooks/08-apache-spark/images/ntlkdata.png\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4zh2_Uc-OvD"
      },
      "source": [
        "# Get the stopwords list in English\n",
        "stopwords_all = []\n",
        "for word in stopwords.words('english'):\n",
        "  stopwords_all.append(word)\n",
        "  stopwords_all.append(word+'_NEG')"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9DFGisz-riT"
      },
      "source": [
        "# Get 10.000 tweets from train dataset and return all words that aren't stopwords\n",
        "ds_train_sample = ds_train.take(10000)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTIgmq0FcBCo"
      },
      "source": [
        "# Mark all negative words with _NEG sufix\n",
        "all_neg_words = sentiment_analyzer.all_words([mark_negation(tweet) for tweet in ds_train_sample])\n",
        "\n",
        "# Get all non-stopwords \n",
        "all_words_neg_nonstops = [x for x in all_neg_words if x not in stopwords_all]"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfCWAs0kecCe"
      },
      "source": [
        "# Create an unigram (n-gram) and extract the features\n",
        "# Get all negative non-stopwords \n",
        "unigram_feats = sentiment_analyzer.unigram_word_feats(all_words_neg_nonstops,top_n=20)\n",
        "# Extract the unigrams\n",
        "sentiment_analyzer.add_feat_extractor(extract_unigram_feats, unigrams = unigram_feats)\n",
        "# Apply features in the train dataset\n",
        "train_set = sentiment_analyzer.apply_features(ds_train_sample)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-7c9O5-kLYI",
        "outputId": "97050887-6e3f-4873-a2b7-700d18e3f78c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Check the data type\n",
        "type(train_set)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nltk.collections.LazyMap"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    }
  ]
}