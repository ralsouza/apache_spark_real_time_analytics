{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_spark_streaming_twitter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1k808iriwznjrjvA82P_caPEP0_ZhuCIR",
      "authorship_tag": "ABX9TyM/65Op+iKauLn0EZHDXZ5Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ralsouza/apache_spark_real_time_analytics/blob/master/notebooks/spark_streaming_twitter/01_spark_streaming_twitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_fEra2hzIM5",
        "colab_type": "text"
      },
      "source": [
        "# PySpark Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QjpiQscwRZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8k0XMjMzzMih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install the dependencies\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FaZqyypzOSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        " \n",
        "# tornar o pyspark \"import√°vel\"\n",
        "import findspark\n",
        "findspark.init('spark-2.4.4-bin-hadoop2.7')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XL0n6A3izPzT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Libraries and Context Setup\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "\n",
        "# Instance Spark Session\n",
        "spark = SparkSession.builder.master('local').appName('My-SparkSQL').getOrCreate()\n",
        "\n",
        "# Create the SQL Context\n",
        "sqlContext = pyspark.SQLContext(sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRPPQv2-zRl9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8b241dd9-9558-43f7-f34f-6fbc952f595f"
      },
      "source": [
        "# Check context\n",
        "print(sc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<SparkContext master=local[*] appName=pyspark-shell>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrnfEnJNeuIY",
        "colab_type": "text"
      },
      "source": [
        "## Other packages to streaming - Twitter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMRD8wO1ereD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install requests_oauthlib\n",
        "!pip install twython\n",
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7_kJi9YfcBE",
        "colab_type": "text"
      },
      "source": [
        "## Install Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RlliEh3e7Yk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.streaming import StreamingContext\n",
        "from requests_oauthlib import OAuth1Session\n",
        "from operator import add\n",
        "from time import gmtime, strftime\n",
        "import requests\n",
        "import time\n",
        "import string \n",
        "import ast"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq6MlEAug0S_",
        "colab_type": "text"
      },
      "source": [
        "## Install NLTK modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0_UEkBrfkYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.sentiment import SentimentAnalyzer\n",
        "from nltk.corpus import subjectivity\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment.util import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qc26fUIAhaFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Update frequency\n",
        "BATCH_INTERVAL = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9RP6bKLhvL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Making the StreamingContext\n",
        "ssc = StreamingContext(sc,batchDuration=BATCH_INTERVAL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRl64bOZ7Oho",
        "colab_type": "text"
      },
      "source": [
        "An essencial part to create a sentiment analysis algorithm, such as any data mining algorithm, is to have a comprehensive data or \"corpus\" to learn, as well as a dataset to test and to ensure it perfectly meet the requeriments.\n",
        "\n",
        "It allows you to adjust the algorithm to deduce better (or more accurate) natural language characteristics that could be extracted from the text and that will contribuite to the sentiment classification, instead of using a generic approach.\n",
        "\n",
        "We will take as a work base a train dataset provided by Michigan University, to Kaggle competitions -  https://inclass.kaggle.com/c/si650winter11.\n",
        "\n",
        "This dataset contains 1.578.627 classified tweets and each row is marked as:\n",
        "* 1 with regard positive sentiment\n",
        "* 0 with regard negative sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf_5VVupnJQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data file path\n",
        "file_path = '/content/drive/My Drive/Colab Notebooks/08-apache-spark/data/sentimentos.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBXlspFHqXdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rdd_sent = sc.textFile(file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Zt0VnJmqB7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removing header\n",
        "header = rdd_sent.take(1)[0]\n",
        "dataset = rdd_sent.filter(lambda row: row != header)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCX5m4QJqSPD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3006db41-053a-4ff6-c678-617e2772da31"
      },
      "source": [
        "type(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.rdd.PipelinedRDD"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h98Fjtz2rKIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function splits the columns in each row, creating a tuple and removing \n",
        "# the punctiation\n",
        "\n",
        "def get_row(row):\n",
        "  row = row.split(\",\")\n",
        "  sentiment = row[1]\n",
        "  tweet = row[3].strip()\n",
        "  translator = str.maketrans({key: None for key in string.punctuation})\n",
        "  tweet = tweet.translate(translator)\n",
        "  tweet = tweet.split(' ')\n",
        "  tweet_lower = ()\n",
        "  for word in tweet:\n",
        "    tweet_lower.append(word.lower())\n",
        "  return (tweet_lower, sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_DeQOVvzVvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply the function in each row in the dataset\n",
        "ds_train = dataset.map(lambda row: get_row(row))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htOq0G8gzmyE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an object SentimentAnalyser\n",
        "sentiment_analyzer = SentimentAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2qz_rH4z53c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}