{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_pyspark_mllib_linear_regression_understanding_the_problem_and_loading_data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMEm5t/b4vkvwVoKoOyuPHO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ralsouza/apache_spark_real_time_analytics/blob/master/notebooks/07_pyspark_mllib_linear_regression_understanding_the_problem_and_loading_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XMKLfM30hSJ"
      },
      "source": [
        "# PySpark Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNb-36kqXqHx"
      },
      "source": [
        "### TESTE OCI Data Science - Useful Tips\n",
        "Everything stored in the <span style=\"background-color: #d5d8dc \">/home/datascience</span> folder is now stored on your block volume drive. The <span style=\"background-color: #d5d8dc \">ads-examples</span> folder has moved outside of your working space. Notebook examples are now accessible through a Launcher tab \"Notebook Examples\" button.\n",
        "<details>\n",
        "<summary><font size=\"2\">1. Check for Public Internet Access</font></summary>\n",
        "\n",
        "```python\n",
        "import requests\n",
        "response = requests.get(\"https://oracle.com\")\n",
        "assert response.status_code==200, \"Internet connection failed\"\n",
        "```\n",
        "</details>\n",
        "<details>\n",
        "<summary><font size=\"2\">2. OCI Configuration and Key Files Set Up</font></summary><p>Follow the instructions in the getting-started notebook. That notebook is accessible via the \"Getting Started\" Launcher tab button.</p>\n",
        "</details>\n",
        "<details>\n",
        "<summary><font size=\"2\">3. Helpful Documentation </font></summary>\n",
        "<ul><li><a href=\"https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm\">Data Science Service Documentation</a></li>\n",
        "<li><a href=\"https://docs.cloud.oracle.com/iaas/tools/ads-sdk/latest/index.html\">ADS documentation</a></li>\n",
        "</ul>\n",
        "</details>\n",
        "<details>\n",
        "<summary><font size=\"2\">4. Typical Cell Imports and Settings</font></summary>\n",
        "\n",
        "```python\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.ERROR)\n",
        "\n",
        "import ads\n",
        "from ads.dataset.factory import DatasetFactory\n",
        "from ads.automl.provider import OracleAutoMLProvider\n",
        "from ads.automl.driver import AutoML\n",
        "from ads.evaluations.evaluator import ADSEvaluator\n",
        "from ads.common.data import MLData\n",
        "from ads.explanations.explainer import ADSExplainer\n",
        "from ads.explanations.mlx_global_explainer import MLXGlobalExplainer\n",
        "from ads.explanations.mlx_local_explainer import MLXLocalExplainer\n",
        "from ads.catalog.model import ModelCatalog\n",
        "from ads.common.model_artifact import ModelArtifact\n",
        "```\n",
        "</details>\n",
        "<details>\n",
        "<summary><font size=\"2\">5. Useful Environment Variables</font></summary>\n",
        "\n",
        "```python\n",
        "import os\n",
        "print(os.environ[\"NB_SESSION_COMPARTMENT_OCID\"])\n",
        "print(os.environ[\"PROJECT_OCID\"])\n",
        "print(os.environ[\"USER_OCID\"])\n",
        "print(os.environ[\"TENANCY_OCID\"])\n",
        "print(os.environ[\"NB_REGION\"])\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4SUqwiLyscR"
      },
      "source": [
        "!apt-get update"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtEiuyW00D8J"
      },
      "source": [
        "# Install the dependencies\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIvVb6LF0FRw"
      },
      "source": [
        "# Environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpO0VHTP0Gws"
      },
      "source": [
        "# Make pyspark \"importable\"\n",
        "import findspark\n",
        "findspark.init('spark-2.4.4-bin-hadoop2.7')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-sa1SVc0N5K"
      },
      "source": [
        "# Libraries and Context Setup\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abhOSTi-0QNq"
      },
      "source": [
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "\n",
        "\n",
        "# Instance Spark Session\n",
        "spark = SparkSession.builder.master('local').appName('My-SparkSQL').getOrCreate()\n",
        "\n",
        "# Create the SQL Context\n",
        "sqlContext = pyspark.SQLContext(sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDF1PTSe0T8o"
      },
      "source": [
        "# Access http://localhost:4040 to see the jobs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}