{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11_pyspark_mllib_clustering_k_means.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPGxRZUR/8uI/8fYfQPUVaj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ralsouza/apache_spark_real_time_analytics/blob/master/notebooks/11_pyspark_mllib_clustering_k_means.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WOFeIAAuyLp"
      },
      "source": [
        "# Clustering K-Means\n",
        "\n",
        "**Description:**\n",
        "*   Unsupervised algorithm;\n",
        "*   Grouping data by simility;\n",
        "*   Partitions the data by a `k` number of clusters, with each observation belongs to a just one cluster;\n",
        "*   The clusterizations is made measuring the distance between the data points and grouping them;\n",
        "*   Multiple distance measures may be used, such as, `Euclidian Distance` and `Manhattan Distance`;\n",
        "\n",
        "**Advantages:**\n",
        "*   Faster;\n",
        "*   Efficient to many variables;\n",
        "\n",
        "**Disvantages:**\n",
        "*   The `k` value must be know;\n",
        "*   The initial value of `k` influences on created clusters;\n",
        "\n",
        "**Application:**\n",
        "*   Priliminary grouping before applying classification techniques;\n",
        "*   Geographic clustering;\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEhesXYmo-Ie"
      },
      "source": [
        "# Setup Spark\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkV_39QPpAz6"
      },
      "source": [
        "!apt-get update"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWrZAuUJpA-e"
      },
      "source": [
        "# Install the dependencies\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYq1kbyjpBHJ"
      },
      "source": [
        "# Environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LCIoFAhp_MR"
      },
      "source": [
        "# Make pyspark \"importable\"\n",
        "import findspark\n",
        "findspark.init('spark-2.4.4-bin-hadoop2.7')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QmgzSkDp_Yl"
      },
      "source": [
        "# Libraries and Context Setup\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv8t6WjWpBUo"
      },
      "source": [
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "\n",
        "\n",
        "# Instance Spark Session\n",
        "spark = SparkSession.builder.master('local').appName('spark_ml_lib').getOrCreate()\n",
        "\n",
        "# Create the SQL Context\n",
        "sqlContext = pyspark.SQLContext(sc)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTIAKHuN0oXV"
      },
      "source": [
        "# Grouping cars"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p01-TxGzuYz-"
      },
      "source": [
        "# Libraries\n",
        "import math\n",
        "import pandas as pd\n",
        "from pyspark.sql import Row\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.clustering import KMeans\n",
        "import matplotlib.pylab as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q2ooZIbzlod"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}